{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "jD22q-a59DcX",
        "outputId": "e9a0288b-1616-413a-b59c-99ad9032c17a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÅ Upload your analysis_prompt_20251120_002408.txt file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-dfb6a658-7092-4d6b-849d-de26023bb0ab\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-dfb6a658-7092-4d6b-849d-de26023bb0ab\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving analysis_prompt_20251120_002408.txt to analysis_prompt_20251120_002408 (1).txt\n",
            "‚úÖ Loaded prompt: 547991 characters\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "# Upload your analysis prompt file\n",
        "print(\"üìÅ Upload your analysis_prompt_20251120_002408.txt file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Read the prompt\n",
        "prompt_file = list(uploaded.keys())[0]\n",
        "with open(prompt_file, 'r', encoding='utf-8') as f:\n",
        "    analysis_prompt = f.read()\n",
        "\n",
        "print(f\"‚úÖ Loaded prompt: {len(analysis_prompt)} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q anthropic google-generativeai groq together huggingface_hub requests"
      ],
      "metadata": {
        "id": "cI0BUweF9iob"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Store your API keys in Colab Secrets (left sidebar ‚Üí üîë Secrets)\n",
        "# Or enter them directly here (less secure):\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')        # Gemini\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')            # Groq (Free)\n",
        "OPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY') # OpenRouter\n",
        "HUGGINGFACE_API_KEY = userdata.get('HUGGINGFACE_API_KEY') # HuggingFace\n",
        "\n",
        "print(\"üîë API Keys loaded!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96oS4v6uAFwA",
        "outputId": "63c3c6f3-fa98-480f-bb31-0963404390c2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üîë API Keys loaded!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q anthropic google-generativeai groq together huggingface_hub requests"
      ],
      "metadata": {
        "id": "I9DzS75yDd_9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import google.generativeai as genai\n",
        "from groq import Groq\n",
        "from huggingface_hub import InferenceClient\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "import time\n",
        "from google.colab import userdata, files\n",
        "\n",
        "# Configure APIs\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "OPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')\n",
        "HUGGINGFACE_API_KEY = userdata.get('HUGGINGFACE_API_KEY')\n",
        "\n",
        "# Initialize clients\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "groq_client = Groq(api_key=GROQ_API_KEY)\n",
        "hf_client = InferenceClient(token=HUGGINGFACE_API_KEY)\n",
        "\n",
        "results = {}\n",
        "errors = {}\n",
        "\n",
        "print(\"üè• MULTI-LLM HOSPITAL ANALYSIS STARTING...\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Function to create a smaller, focused prompt\n",
        "def create_focused_prompt(full_data):\n",
        "    \"\"\"Create a smaller prompt by summarizing data\"\"\"\n",
        "\n",
        "    # Extract just the key information\n",
        "    hospitals = list(full_data['data'].keys())\n",
        "\n",
        "    # Build a condensed version with sample reviews only\n",
        "    condensed_data = {}\n",
        "    for hospital, data in full_data['data'].items():\n",
        "        # Take only first 10 reviews and 5 news items as samples\n",
        "        condensed_data[hospital] = {\n",
        "            'review_count': data['review_count'],\n",
        "            'news_count': data['news_count'],\n",
        "            'sample_reviews': data['reviews'][:10],  # First 10 reviews\n",
        "            'sample_news': data['news'][:5]  # First 5 news items\n",
        "        }\n",
        "\n",
        "    prompt = f\"\"\"I have collected review and news data for {len(hospitals)} hospitals in Delhi:\n",
        "{', '.join(hospitals)}\n",
        "\n",
        "DATA SUMMARY:\n",
        "{chr(10).join([f\"{h}: {condensed_data[h]['review_count']} reviews, {condensed_data[h]['news_count']} news\" for h in hospitals])}\n",
        "\n",
        "SAMPLE DATA (showing first 10 reviews and 5 news per hospital):\n",
        "{json.dumps(condensed_data, indent=2)}\n",
        "\n",
        "Based on this sample data, provide:\n",
        "\n",
        "A. OVERALL RANKING (1-{len(hospitals)}):\n",
        "- Rank hospitals best to worst\n",
        "- Give each a score out of 10\n",
        "- Justify with specific review quotes in [brackets]\n",
        "\n",
        "B. TOP 3 STRENGTHS & WEAKNESSES for each hospital:\n",
        "- Use actual review quotes as evidence\n",
        "\n",
        "C. BEST HOSPITAL FOR:\n",
        "- Emergency care\n",
        "- Planned surgery\n",
        "- Budget patients\n",
        "- Cancer treatment\n",
        "- Heart disease\n",
        "- Orthopedics\n",
        "\n",
        "D. KEY RECOMMENDATIONS:\n",
        "- Which hospitals to prefer for specific needs\n",
        "- Which to avoid and why\n",
        "\n",
        "Be specific and quote actual reviews in [brackets].\"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Load your full data\n",
        "print(\"üìÅ Upload your combined hospital data JSON file:\")\n",
        "uploaded = files.upload()\n",
        "data_file = list(uploaded.keys())[0]\n",
        "\n",
        "with open(data_file, 'r', encoding='utf-8') as f:\n",
        "    full_hospital_data = json.load(f)\n",
        "\n",
        "# Create focused prompt\n",
        "analysis_prompt = create_focused_prompt(full_hospital_data)\n",
        "print(f\"\\n‚úÖ Created focused prompt: ~{len(analysis_prompt)} characters\")\n",
        "print(f\"Estimated tokens: ~{len(analysis_prompt.split())}\")\n",
        "\n",
        "# 1. GEMINI 1.5 FLASH (Fallback to older model with higher quota)\n",
        "try:\n",
        "    print(\"\\nü§ñ Running: Gemini 1.5 Flash...\")\n",
        "    start = time.time()\n",
        "\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "    response = model.generate_content(analysis_prompt)\n",
        "\n",
        "    results['gemini_flash'] = {\n",
        "        'response': response.text,\n",
        "        'time': time.time() - start,\n",
        "        'model': 'Gemini 1.5 Flash (Google)'\n",
        "    }\n",
        "    print(f\"‚úÖ Gemini Flash completed in {results['gemini_flash']['time']:.2f}s\")\n",
        "\n",
        "except Exception as e:\n",
        "    errors['gemini_flash'] = str(e)\n",
        "    print(f\"‚ùå Gemini Flash failed: {e}\")\n",
        "\n",
        "# 2. GROQ - Llama 3.1 70B Versatile (Updated model)\n",
        "try:\n",
        "    print(\"\\nü§ñ Running: Groq Llama 3.1 70B...\")\n",
        "    start = time.time()\n",
        "\n",
        "    response = groq_client.chat.completions.create(\n",
        "        model=\"llama-3.1-70b-versatile\",\n",
        "        messages=[{\"role\": \"user\", \"content\": analysis_prompt}],\n",
        "        temperature=0.7,\n",
        "        max_tokens=4000\n",
        "    )\n",
        "\n",
        "    results['groq_llama_70b'] = {\n",
        "        'response': response.choices[0].message.content,\n",
        "        'time': time.time() - start,\n",
        "        'model': 'Llama 3.1 70B (Groq)'\n",
        "    }\n",
        "    print(f\"‚úÖ Groq Llama 70B completed in {results['groq_llama_70b']['time']:.2f}s\")\n",
        "\n",
        "except Exception as e:\n",
        "    errors['groq_llama_70b'] = str(e)\n",
        "    print(f\"‚ùå Groq Llama 70B failed: {e}\")\n",
        "\n",
        "# 3. GROQ - Llama 3.2 90B (Newer model)\n",
        "try:\n",
        "    print(\"\\nü§ñ Running: Groq Llama 3.2 90B...\")\n",
        "    start = time.time()\n",
        "\n",
        "    response = groq_client.chat.completions.create(\n",
        "        model=\"llama-3.2-90b-text-preview\",\n",
        "        messages=[{\"role\": \"user\", \"content\": analysis_prompt}],\n",
        "        temperature=0.7,\n",
        "        max_tokens=4000\n",
        "    )\n",
        "\n",
        "    results['groq_llama_90b'] = {\n",
        "        'response': response.choices[0].message.content,\n",
        "        'time': time.time() - start,\n",
        "        'model': 'Llama 3.2 90B (Groq)'\n",
        "    }\n",
        "    print(f\"‚úÖ Groq Llama 90B completed in {results['groq_llama_90b']['time']:.2f}s\")\n",
        "\n",
        "except Exception as e:\n",
        "    errors['groq_llama_90b'] = str(e)\n",
        "    print(f\"‚ùå Groq Llama 90B failed: {e}\")\n",
        "\n",
        "# 4. OPENROUTER - Claude 3.5 Haiku\n",
        "try:\n",
        "    print(\"\\nü§ñ Running: OpenRouter Claude 3.5 Haiku...\")\n",
        "    start = time.time()\n",
        "\n",
        "    response = requests.post(\n",
        "        url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
        "        headers={\n",
        "            \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"HTTP-Referer\": \"https://github.com/yourusername\",\n",
        "            \"X-Title\": \"Hospital Analysis\"\n",
        "        },\n",
        "        json={\n",
        "            \"model\": \"anthropic/claude-3.5-haiku\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": analysis_prompt}],\n",
        "            \"max_tokens\": 4000\n",
        "        },\n",
        "        timeout=120\n",
        "    )\n",
        "\n",
        "    response.raise_for_status()\n",
        "    response_data = response.json()\n",
        "\n",
        "    results['openrouter_claude'] = {\n",
        "        'response': response_data['choices'][0]['message']['content'],\n",
        "        'time': time.time() - start,\n",
        "        'model': 'Claude 3.5 Haiku (OpenRouter)'\n",
        "    }\n",
        "    print(f\"‚úÖ OpenRouter Claude completed in {results['openrouter_claude']['time']:.2f}s\")\n",
        "\n",
        "except Exception as e:\n",
        "    errors['openrouter_claude'] = str(e)\n",
        "    print(f\"‚ùå OpenRouter Claude failed: {e}\")\n",
        "\n",
        "# 5. OPENROUTER - Meta Llama 3.1 405B\n",
        "try:\n",
        "    print(\"\\nü§ñ Running: OpenRouter Llama 405B...\")\n",
        "    start = time.time()\n",
        "\n",
        "    response = requests.post(\n",
        "        url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
        "        headers={\n",
        "            \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "            \"Content-Type\": \"application/json\",\n",
        "            \"HTTP-Referer\": \"https://github.com/yourusername\",\n",
        "            \"X-Title\": \"Hospital Analysis\"\n",
        "        },\n",
        "        json={\n",
        "            \"model\": \"meta-llama/llama-3.1-405b-instruct\",\n",
        "            \"messages\": [{\"role\": \"user\", \"content\": analysis_prompt}],\n",
        "            \"max_tokens\": 4000\n",
        "        },\n",
        "        timeout=120\n",
        "    )\n",
        "\n",
        "    response.raise_for_status()\n",
        "    response_data = response.json()\n",
        "\n",
        "    results['openrouter_llama405b'] = {\n",
        "        'response': response_data['choices'][0]['message']['content'],\n",
        "        'time': time.time() - start,\n",
        "        'model': 'Llama 3.1 405B (OpenRouter)'\n",
        "    }\n",
        "    print(f\"‚úÖ OpenRouter Llama 405B completed in {results['openrouter_llama405b']['time']:.2f}s\")\n",
        "\n",
        "except Exception as e:\n",
        "    errors['openrouter_llama405b'] = str(e)\n",
        "    print(f\"‚ùå OpenRouter Llama 405B failed: {e}\")\n",
        "\n",
        "# 6. HUGGINGFACE - Qwen 2.5 72B\n",
        "try:\n",
        "    print(\"\\nü§ñ Running: HuggingFace Qwen 2.5 72B...\")\n",
        "    start = time.time()\n",
        "\n",
        "    response = hf_client.chat_completion(\n",
        "        model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "        messages=[{\"role\": \"user\", \"content\": analysis_prompt}],\n",
        "        max_tokens=4000\n",
        "    )\n",
        "\n",
        "    results['hf_qwen'] = {\n",
        "        'response': response.choices[0].message.content,\n",
        "        'time': time.time() - start,\n",
        "        'model': 'Qwen 2.5 72B (HuggingFace)'\n",
        "    }\n",
        "    print(f\"‚úÖ HuggingFace Qwen completed in {results['hf_qwen']['time']:.2f}s\")\n",
        "\n",
        "except Exception as e:\n",
        "    errors['hf_qwen'] = str(e)\n",
        "    print(f\"‚ùå HuggingFace Qwen failed: {e}\")\n",
        "\n",
        "# 7. HUGGINGFACE - Meta Llama 3.1 70B\n",
        "try:\n",
        "    print(\"\\nü§ñ Running: HuggingFace Llama 3.1 70B...\")\n",
        "    start = time.time()\n",
        "\n",
        "    response = hf_client.chat_completion(\n",
        "        model=\"meta-llama/Llama-3.1-70B-Instruct\",\n",
        "        messages=[{\"role\": \"user\", \"content\": analysis_prompt}],\n",
        "        max_tokens=4000\n",
        "    )\n",
        "\n",
        "    results['hf_llama'] = {\n",
        "        'response': response.choices[0].message.content,\n",
        "        'time': time.time() - start,\n",
        "        'model': 'Llama 3.1 70B (HuggingFace)'\n",
        "    }\n",
        "    print(f\"‚úÖ HuggingFace Llama completed in {results['hf_llama']['time']:.2f}s\")\n",
        "\n",
        "except Exception as e:\n",
        "    errors['hf_llama'] = str(e)\n",
        "    print(f\"‚ùå HuggingFace Llama failed: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"üìä ANALYSIS COMPLETE!\")\n",
        "print(f\"‚úÖ Successful: {len(results)} models\")\n",
        "print(f\"‚ùå Failed: {len(errors)} models\")\n",
        "\n",
        "if errors:\n",
        "    print(\"\\n‚ö†Ô∏è Failed models:\")\n",
        "    for model, error in errors.items():\n",
        "        print(f\"  - {model}: {error[:100]}...\")\n",
        "\n",
        "# Save results\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "output_data = {\n",
        "    'timestamp': timestamp,\n",
        "    'successful_models': len(results),\n",
        "    'failed_models': len(errors),\n",
        "    'results': results,\n",
        "    'errors': errors,\n",
        "    'prompt_used': analysis_prompt\n",
        "}\n",
        "\n",
        "output_file = f'hospital_analysis_{timestamp}.json'\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\nüíæ Results saved to: {output_file}\")\n",
        "files.download(output_file)\n",
        "\n",
        "# Save individual markdown reports\n",
        "for model_name, data in results.items():\n",
        "    filename = f\"{model_name}_analysis.md\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"# Hospital Analysis - {data['model']}\\n\\n\")\n",
        "        f.write(f\"**Generated:** {timestamp}\\n\")\n",
        "        f.write(f\"**Processing Time:** {data['time']:.2f}s\\n\\n\")\n",
        "        f.write(\"---\\n\\n\")\n",
        "        f.write(data['response'])\n",
        "    print(f\"üìÑ Saved: {filename}\")\n",
        "    files.download(filename)\n",
        "\n",
        "# Print preview\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESULTS PREVIEW\")\n",
        "print(\"=\"*60)\n",
        "for model_name, data in results.items():\n",
        "    print(f\"\\nü§ñ {data['model']}\")\n",
        "    print(f\"‚è±Ô∏è  {data['time']:.2f}s\")\n",
        "    print(\"-\"*60)\n",
        "    print(data['response'][:400] + \"...\\n\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kO5BlaegDfR7",
        "outputId": "dd45853d-90ce-4ec8-b6f5-fa298d9f2268"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üè• MULTI-LLM HOSPITAL ANALYSIS STARTING...\n",
            "\n",
            "============================================================\n",
            "üìÅ Upload your combined hospital data JSON file:\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ],
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7220fc83-5925-460f-8cd7-8456cf38d179\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7220fc83-5925-460f-8cd7-8456cf38d179\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Saving all_hospitals_combined_20251120_002403.json to all_hospitals_combined_20251120_002403.json\n",
            "\n",
            "‚úÖ Created focused prompt: ~64140 characters\n",
            "Estimated tokens: ~9591\n",
            "\n",
            "ü§ñ Running: Gemini 1.5 Flash...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1498.11ms\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå Gemini Flash failed: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
            "\n",
            "ü§ñ Running: Groq Llama 3.1 70B...\n",
            "‚ùå Groq Llama 70B failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
            "\n",
            "ü§ñ Running: Groq Llama 3.2 90B...\n",
            "‚ùå Groq Llama 90B failed: Error code: 400 - {'error': {'message': 'The model `llama-3.2-90b-text-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
            "\n",
            "ü§ñ Running: OpenRouter Claude 3.5 Haiku...\n",
            "‚úÖ OpenRouter Claude completed in 20.37s\n",
            "\n",
            "ü§ñ Running: OpenRouter Llama 405B...\n",
            "‚ùå OpenRouter Llama 405B failed: HTTPSConnectionPool(host='openrouter.ai', port=443): Read timed out. (read timeout=120)\n",
            "\n",
            "ü§ñ Running: HuggingFace Qwen 2.5 72B...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Model meta-llama/Llama-3.1-70B-Instruct is in staging mode for provider hyperbolic. Meant for test purposes only.\n",
            "WARNING:huggingface_hub.inference._providers._common:Model meta-llama/Llama-3.1-70B-Instruct is in staging mode for provider hyperbolic. Meant for test purposes only.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "‚ùå HuggingFace Qwen failed: 504 Server Error: Gateway Time-out for url: https://router.huggingface.co/novita/v3/openai/chat/completions\n",
            "\n",
            "ü§ñ Running: HuggingFace Llama 3.1 70B...\n",
            "‚úÖ HuggingFace Llama completed in 39.26s\n",
            "\n",
            "============================================================\n",
            "üìä ANALYSIS COMPLETE!\n",
            "‚úÖ Successful: 2 models\n",
            "‚ùå Failed: 5 models\n",
            "\n",
            "‚ö†Ô∏è Failed models:\n",
            "  - gemini_flash: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%2...\n",
            "  - groq_llama_70b: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned...\n",
            "  - groq_llama_90b: Error code: 400 - {'error': {'message': 'The model `llama-3.2-90b-text-preview` has been decommissio...\n",
            "  - openrouter_llama405b: HTTPSConnectionPool(host='openrouter.ai', port=443): Read timed out. (read timeout=120)...\n",
            "  - hf_qwen: 504 Server Error: Gateway Time-out for url: https://router.huggingface.co/novita/v3/openai/chat/comp...\n",
            "\n",
            "üíæ Results saved to: hospital_analysis_20251121_062544.json\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_7cd24daf-6e24-4ec6-8cf8-7b3a1d4207f9\", \"hospital_analysis_20251121_062544.json\", 78117)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Saved: openrouter_claude_analysis.md\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_4b5e324c-fbb8-4fd3-ad23-7851756b78f8\", \"openrouter_claude_analysis.md\", 3145)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "üìÑ Saved: hf_llama_analysis.md\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_1167f6d0-51bb-4e88-b626-4b093d8fd6f8\", \"hf_llama_analysis.md\", 6666)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "============================================================\n",
            "RESULTS PREVIEW\n",
            "============================================================\n",
            "\n",
            "ü§ñ Claude 3.5 Haiku (OpenRouter)\n",
            "‚è±Ô∏è  20.37s\n",
            "------------------------------------------------------------\n",
            "A. OVERALL RANKING (Out of 10):\n",
            "\n",
            "1. AIIMS (8.5/10)\n",
            "- Consistently positive reviews\n",
            "- [Cutting edge technology]\n",
            "- [Internationally renowned doctors]\n",
            "- [High-quality care at reasonable prices]\n",
            "\n",
            "2. Max (7/10)\n",
            "- Mixed reviews with some excellent experiences\n",
            "- [Professional and caring staff in some departments]\n",
            "- Some serious patient care concerns\n",
            "\n",
            "3. Apollo (6.5/10)\n",
            "- Varied experiences\n",
            "- [Good for in...\n",
            "\n",
            "\n",
            "ü§ñ Llama 3.1 70B (HuggingFace)\n",
            "‚è±Ô∏è  39.26s\n",
            "------------------------------------------------------------\n",
            "A. OVERALL RANKING (1-7):\n",
            "\n",
            "1. AIIMS - Score: 8.5/10 [Reviews consistently mention excellent treatment, good doctors, and high-quality facilities. \"Aiims Hospital, Madurai ensures that you get the highest quality of care and it has top-notch facilities and amenities.\"]\n",
            "\n",
            "2. Max - Score: 7.5/10 [Reviews praise the hospital's cleanliness, professionalism, and care. However, some reviews mention poor s...\n",
            "\n"
          ]
        }
      ]
    }
  ]
}