{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 110
        },
        "id": "jD22q-a59DcX",
        "outputId": "404f7671-81b6-49c5-8cbe-fd0081fe7008"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“ Upload your analysis_prompt_20251120_002408.txt file:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-aca4cda2-8415-4b70-a0da-c34c7cddcc7e\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-aca4cda2-8415-4b70-a0da-c34c7cddcc7e\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving analysis_prompt_20251120_002408.txt to analysis_prompt_20251120_002408.txt\n",
            "âœ… Loaded prompt: 547991 characters\n"
          ]
        }
      ],
      "source": [
        "from google.colab import files\n",
        "import json\n",
        "\n",
        "# Upload your analysis prompt file\n",
        "print(\"ğŸ“ Upload your analysis_prompt_20251120_002408.txt file:\")\n",
        "uploaded = files.upload()\n",
        "\n",
        "# Read the prompt\n",
        "prompt_file = list(uploaded.keys())[0]\n",
        "with open(prompt_file, 'r', encoding='utf-8') as f:\n",
        "    analysis_prompt = f.read()\n",
        "\n",
        "print(f\"âœ… Loaded prompt: {len(analysis_prompt)} characters\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cI0BUweF9iob",
        "outputId": "a22aa1d3-a655-4bba-87a2-707303901f93"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m85.2/85.2 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m388.2/388.2 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m137.3/137.3 kB\u001b[0m \u001b[31m5.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m114.5/114.5 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m1.7/1.7 MB\u001b[0m \u001b[31m28.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m46.7/46.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ],
      "source": [
        "!pip install -q anthropic google-generativeai groq together huggingface_hub requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "96oS4v6uAFwA",
        "outputId": "61cdbac3-2b5c-4d88-d661-3774cbd9638c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ”‘ API Keys loaded!\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from google.colab import userdata\n",
        "\n",
        "# Store your API keys in Colab Secrets (left sidebar â†’ ğŸ”‘ Secrets)\n",
        "# Or enter them directly here (less secure):\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')        # Gemini\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')            # Groq (Free)\n",
        "OPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY') # OpenRouter\n",
        "HUGGINGFACE_API_KEY = userdata.get('HUGGINGFACE_API_KEY') # HuggingFace\n",
        "\n",
        "print(\"ğŸ”‘ API Keys loaded!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "I9DzS75yDd_9"
      },
      "outputs": [],
      "source": [
        "!pip install -q anthropic google-generativeai groq together huggingface_hub requests"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "kO5BlaegDfR7",
        "outputId": "079eecf8-5fcd-4cf8-9767-c8fa2c22c577"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ¥ MULTI-LLM HOSPITAL ANALYSIS STARTING...\n",
            "\n",
            "============================================================\n",
            "ğŸ“ Upload your combined hospital data JSON file:\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "     <input type=\"file\" id=\"files-7bd23562-ad42-4d63-8e51-ee068af89b31\" name=\"files[]\" multiple disabled\n",
              "        style=\"border:none\" />\n",
              "     <output id=\"result-7bd23562-ad42-4d63-8e51-ee068af89b31\">\n",
              "      Upload widget is only available when the cell has been executed in the\n",
              "      current browser session. Please rerun this cell to enable.\n",
              "      </output>\n",
              "      <script>// Copyright 2017 Google LLC\n",
              "//\n",
              "// Licensed under the Apache License, Version 2.0 (the \"License\");\n",
              "// you may not use this file except in compliance with the License.\n",
              "// You may obtain a copy of the License at\n",
              "//\n",
              "//      http://www.apache.org/licenses/LICENSE-2.0\n",
              "//\n",
              "// Unless required by applicable law or agreed to in writing, software\n",
              "// distributed under the License is distributed on an \"AS IS\" BASIS,\n",
              "// WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
              "// See the License for the specific language governing permissions and\n",
              "// limitations under the License.\n",
              "\n",
              "/**\n",
              " * @fileoverview Helpers for google.colab Python module.\n",
              " */\n",
              "(function(scope) {\n",
              "function span(text, styleAttributes = {}) {\n",
              "  const element = document.createElement('span');\n",
              "  element.textContent = text;\n",
              "  for (const key of Object.keys(styleAttributes)) {\n",
              "    element.style[key] = styleAttributes[key];\n",
              "  }\n",
              "  return element;\n",
              "}\n",
              "\n",
              "// Max number of bytes which will be uploaded at a time.\n",
              "const MAX_PAYLOAD_SIZE = 100 * 1024;\n",
              "\n",
              "function _uploadFiles(inputId, outputId) {\n",
              "  const steps = uploadFilesStep(inputId, outputId);\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  // Cache steps on the outputElement to make it available for the next call\n",
              "  // to uploadFilesContinue from Python.\n",
              "  outputElement.steps = steps;\n",
              "\n",
              "  return _uploadFilesContinue(outputId);\n",
              "}\n",
              "\n",
              "// This is roughly an async generator (not supported in the browser yet),\n",
              "// where there are multiple asynchronous steps and the Python side is going\n",
              "// to poll for completion of each step.\n",
              "// This uses a Promise to block the python side on completion of each step,\n",
              "// then passes the result of the previous step as the input to the next step.\n",
              "function _uploadFilesContinue(outputId) {\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  const steps = outputElement.steps;\n",
              "\n",
              "  const next = steps.next(outputElement.lastPromiseValue);\n",
              "  return Promise.resolve(next.value.promise).then((value) => {\n",
              "    // Cache the last promise value to make it available to the next\n",
              "    // step of the generator.\n",
              "    outputElement.lastPromiseValue = value;\n",
              "    return next.value.response;\n",
              "  });\n",
              "}\n",
              "\n",
              "/**\n",
              " * Generator function which is called between each async step of the upload\n",
              " * process.\n",
              " * @param {string} inputId Element ID of the input file picker element.\n",
              " * @param {string} outputId Element ID of the output display.\n",
              " * @return {!Iterable<!Object>} Iterable of next steps.\n",
              " */\n",
              "function* uploadFilesStep(inputId, outputId) {\n",
              "  const inputElement = document.getElementById(inputId);\n",
              "  inputElement.disabled = false;\n",
              "\n",
              "  const outputElement = document.getElementById(outputId);\n",
              "  outputElement.innerHTML = '';\n",
              "\n",
              "  const pickedPromise = new Promise((resolve) => {\n",
              "    inputElement.addEventListener('change', (e) => {\n",
              "      resolve(e.target.files);\n",
              "    });\n",
              "  });\n",
              "\n",
              "  const cancel = document.createElement('button');\n",
              "  inputElement.parentElement.appendChild(cancel);\n",
              "  cancel.textContent = 'Cancel upload';\n",
              "  const cancelPromise = new Promise((resolve) => {\n",
              "    cancel.onclick = () => {\n",
              "      resolve(null);\n",
              "    };\n",
              "  });\n",
              "\n",
              "  // Wait for the user to pick the files.\n",
              "  const files = yield {\n",
              "    promise: Promise.race([pickedPromise, cancelPromise]),\n",
              "    response: {\n",
              "      action: 'starting',\n",
              "    }\n",
              "  };\n",
              "\n",
              "  cancel.remove();\n",
              "\n",
              "  // Disable the input element since further picks are not allowed.\n",
              "  inputElement.disabled = true;\n",
              "\n",
              "  if (!files) {\n",
              "    return {\n",
              "      response: {\n",
              "        action: 'complete',\n",
              "      }\n",
              "    };\n",
              "  }\n",
              "\n",
              "  for (const file of files) {\n",
              "    const li = document.createElement('li');\n",
              "    li.append(span(file.name, {fontWeight: 'bold'}));\n",
              "    li.append(span(\n",
              "        `(${file.type || 'n/a'}) - ${file.size} bytes, ` +\n",
              "        `last modified: ${\n",
              "            file.lastModifiedDate ? file.lastModifiedDate.toLocaleDateString() :\n",
              "                                    'n/a'} - `));\n",
              "    const percent = span('0% done');\n",
              "    li.appendChild(percent);\n",
              "\n",
              "    outputElement.appendChild(li);\n",
              "\n",
              "    const fileDataPromise = new Promise((resolve) => {\n",
              "      const reader = new FileReader();\n",
              "      reader.onload = (e) => {\n",
              "        resolve(e.target.result);\n",
              "      };\n",
              "      reader.readAsArrayBuffer(file);\n",
              "    });\n",
              "    // Wait for the data to be ready.\n",
              "    let fileData = yield {\n",
              "      promise: fileDataPromise,\n",
              "      response: {\n",
              "        action: 'continue',\n",
              "      }\n",
              "    };\n",
              "\n",
              "    // Use a chunked sending to avoid message size limits. See b/62115660.\n",
              "    let position = 0;\n",
              "    do {\n",
              "      const length = Math.min(fileData.byteLength - position, MAX_PAYLOAD_SIZE);\n",
              "      const chunk = new Uint8Array(fileData, position, length);\n",
              "      position += length;\n",
              "\n",
              "      const base64 = btoa(String.fromCharCode.apply(null, chunk));\n",
              "      yield {\n",
              "        response: {\n",
              "          action: 'append',\n",
              "          file: file.name,\n",
              "          data: base64,\n",
              "        },\n",
              "      };\n",
              "\n",
              "      let percentDone = fileData.byteLength === 0 ?\n",
              "          100 :\n",
              "          Math.round((position / fileData.byteLength) * 100);\n",
              "      percent.textContent = `${percentDone}% done`;\n",
              "\n",
              "    } while (position < fileData.byteLength);\n",
              "  }\n",
              "\n",
              "  // All done.\n",
              "  yield {\n",
              "    response: {\n",
              "      action: 'complete',\n",
              "    }\n",
              "  };\n",
              "}\n",
              "\n",
              "scope.google = scope.google || {};\n",
              "scope.google.colab = scope.google.colab || {};\n",
              "scope.google.colab._files = {\n",
              "  _uploadFiles,\n",
              "  _uploadFilesContinue,\n",
              "};\n",
              "})(self);\n",
              "</script> "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Saving all_hospitals_combined_20251120_002403.json to all_hospitals_combined_20251120_002403.json\n",
            "\n",
            "âœ… Created focused prompt: ~64140 characters\n",
            "Estimated tokens: ~9591\n",
            "\n",
            "ğŸ¤– Running: Gemini 1.5 Flash...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING:tornado.access:404 POST /v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint (::1) 1446.83ms\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âŒ Gemini Flash failed: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%24alt=json%3Benum-encoding%3Dint: models/gemini-1.5-flash is not found for API version v1beta, or is not supported for generateContent. Call ListModels to see the list of available models and their supported methods.\n",
            "\n",
            "ğŸ¤– Running: Groq Llama 3.1 70B...\n",
            "âŒ Groq Llama 70B failed: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
            "\n",
            "ğŸ¤– Running: Groq Llama 3.2 90B...\n",
            "âŒ Groq Llama 90B failed: Error code: 400 - {'error': {'message': 'The model `llama-3.2-90b-text-preview` has been decommissioned and is no longer supported. Please refer to https://console.groq.com/docs/deprecations for a recommendation on which model to use instead.', 'type': 'invalid_request_error', 'code': 'model_decommissioned'}}\n",
            "\n",
            "============================================================\n",
            "ğŸŒ OPENROUTER MODELS\n",
            "============================================================\n",
            "\n",
            "ğŸ¤– Running: Claude 3.5 Sonnet (OpenRouter)...\n",
            "âŒ Claude 3.5 Sonnet (OpenRouter) failed: 402 Client Error: Payment Required for url: https://openrouter.ai/api/v1/chat/completions\n",
            "\n",
            "ğŸ¤– Running: Claude 3.5 Haiku (OpenRouter)...\n",
            "âœ… Claude 3.5 Haiku (OpenRouter) completed in 18.18s\n",
            "\n",
            "ğŸ¤– Running: Claude 3 Opus (OpenRouter)...\n",
            "âŒ Claude 3 Opus (OpenRouter) failed: 402 Client Error: Payment Required for url: https://openrouter.ai/api/v1/chat/completions\n",
            "\n",
            "ğŸ¤– Running: Claude 3 Sonnet (OpenRouter)...\n",
            "âŒ Claude 3 Sonnet (OpenRouter) failed: 404 Client Error: Not Found for url: https://openrouter.ai/api/v1/chat/completions\n",
            "\n",
            "ğŸ¤– Running: Llama 3.1 405B (OpenRouter)...\n",
            "âŒ Llama 3.1 405B (OpenRouter) failed: 402 Client Error: Payment Required for url: https://openrouter.ai/api/v1/chat/completions\n",
            "\n",
            "ğŸ¤– Running: Llama 3.1 70B (OpenRouter)...\n",
            "âœ… Llama 3.1 70B (OpenRouter) completed in 16.97s\n",
            "\n",
            "ğŸ¤– Running: Llama 3.3 70B (OpenRouter)...\n",
            "âœ… Llama 3.3 70B (OpenRouter) completed in 46.88s\n",
            "\n",
            "ğŸ¤– Running: Gemini Pro 1.5 (OpenRouter)...\n",
            "âŒ Gemini Pro 1.5 (OpenRouter) failed: 404 Client Error: Not Found for url: https://openrouter.ai/api/v1/chat/completions\n",
            "\n",
            "ğŸ¤– Running: Gemini Flash 1.5 (OpenRouter)...\n",
            "âŒ Gemini Flash 1.5 (OpenRouter) failed: 404 Client Error: Not Found for url: https://openrouter.ai/api/v1/chat/completions\n",
            "\n",
            "ğŸ¤– Running: GPT-4 Turbo (OpenRouter)...\n",
            "âŒ GPT-4 Turbo (OpenRouter) failed: 402 Client Error: Payment Required for url: https://openrouter.ai/api/v1/chat/completions\n",
            "\n",
            "ğŸ¤– Running: GPT-4o (OpenRouter)...\n",
            "âŒ GPT-4o (OpenRouter) failed: 402 Client Error: Payment Required for url: https://openrouter.ai/api/v1/chat/completions\n",
            "\n",
            "ğŸ¤– Running: GPT-4o Mini (OpenRouter)...\n",
            "âœ… GPT-4o Mini (OpenRouter) completed in 39.19s\n",
            "\n",
            "ğŸ¤– Running: Mistral Large (OpenRouter)...\n",
            "âŒ Mistral Large (OpenRouter) failed: 402 Client Error: Payment Required for url: https://openrouter.ai/api/v1/chat/completions\n",
            "\n",
            "ğŸ¤– Running: Mistral Medium (OpenRouter)...\n",
            "âŒ Mistral Medium (OpenRouter) failed: 404 Client Error: Not Found for url: https://openrouter.ai/api/v1/chat/completions\n",
            "\n",
            "ğŸ¤– Running: Qwen 2.5 72B (OpenRouter)...\n",
            "âœ… Qwen 2.5 72B (OpenRouter) completed in 91.20s\n",
            "\n",
            "ğŸ¤– Running: DeepSeek Chat (OpenRouter)...\n",
            "âœ… DeepSeek Chat (OpenRouter) completed in 52.02s\n",
            "\n",
            "ğŸ¤– Running: Cohere Command R+ (OpenRouter)...\n",
            "âŒ Cohere Command R+ (OpenRouter) failed: 404 Client Error: Not Found for url: https://openrouter.ai/api/v1/chat/completions\n",
            "\n",
            "ğŸ¤– Running: Perplexity Sonar Large (OpenRouter)...\n",
            "âŒ Perplexity Sonar Large (OpenRouter) failed: 404 Client Error: Not Found for url: https://openrouter.ai/api/v1/chat/completions\n",
            "\n",
            "ğŸ¤– Running: HuggingFace Qwen 2.5 72B...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n",
            "Model meta-llama/Llama-3.1-70B-Instruct is in staging mode for provider hyperbolic. Meant for test purposes only.\n",
            "WARNING:huggingface_hub.inference._providers._common:Model meta-llama/Llama-3.1-70B-Instruct is in staging mode for provider hyperbolic. Meant for test purposes only.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "âŒ HuggingFace Qwen failed: 504 Server Error: Gateway Time-out for url: https://router.huggingface.co/novita/v3/openai/chat/completions\n",
            "\n",
            "ğŸ¤– Running: HuggingFace Llama 3.1 70B...\n",
            "âœ… HuggingFace Llama completed in 11.54s\n",
            "\n",
            "============================================================\n",
            "ğŸ“Š ANALYSIS COMPLETE!\n",
            "âœ… Successful: 7 models\n",
            "âŒ Failed: 16 models\n",
            "\n",
            "âš ï¸ Failed models:\n",
            "  - gemini_flash: 404 POST https://generativelanguage.googleapis.com/v1beta/models/gemini-1.5-flash:generateContent?%2...\n",
            "  - groq_llama_70b: Error code: 400 - {'error': {'message': 'The model `llama-3.1-70b-versatile` has been decommissioned...\n",
            "  - groq_llama_90b: Error code: 400 - {'error': {'message': 'The model `llama-3.2-90b-text-preview` has been decommissio...\n",
            "  - openrouter_claude_3_5_sonnet: 402 Client Error: Payment Required for url: https://openrouter.ai/api/v1/chat/completions...\n",
            "  - openrouter_claude_3_opus: 402 Client Error: Payment Required for url: https://openrouter.ai/api/v1/chat/completions...\n",
            "  - openrouter_claude_3_sonnet: 404 Client Error: Not Found for url: https://openrouter.ai/api/v1/chat/completions...\n",
            "  - openrouter_llama_3_1_405b: 402 Client Error: Payment Required for url: https://openrouter.ai/api/v1/chat/completions...\n",
            "  - openrouter_gemini_pro_1_5: 404 Client Error: Not Found for url: https://openrouter.ai/api/v1/chat/completions...\n",
            "  - openrouter_gemini_flash_1_5: 404 Client Error: Not Found for url: https://openrouter.ai/api/v1/chat/completions...\n",
            "  - openrouter_gpt_4_turbo: 402 Client Error: Payment Required for url: https://openrouter.ai/api/v1/chat/completions...\n",
            "  - openrouter_gpt_4o: 402 Client Error: Payment Required for url: https://openrouter.ai/api/v1/chat/completions...\n",
            "  - openrouter_mistral_large: 402 Client Error: Payment Required for url: https://openrouter.ai/api/v1/chat/completions...\n",
            "  - openrouter_mistral_medium: 404 Client Error: Not Found for url: https://openrouter.ai/api/v1/chat/completions...\n",
            "  - openrouter_cohere_command_rplus: 404 Client Error: Not Found for url: https://openrouter.ai/api/v1/chat/completions...\n",
            "  - openrouter_perplexity_sonar_large: 404 Client Error: Not Found for url: https://openrouter.ai/api/v1/chat/completions...\n",
            "  - hf_qwen: 504 Server Error: Gateway Time-out for url: https://router.huggingface.co/novita/v3/openai/chat/comp...\n",
            "\n",
            "ğŸ’¾ Results saved to: hospital_analysis_20251126_125044.json\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_4bc698f0-6e33-4cf1-9917-a1819f9d5a38\", \"hospital_analysis_20251126_125044.json\", 121284)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“„ Saved: openrouter_claude_3_5_haiku_analysis.md\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_e09e9e74-330e-45bf-83a2-c1f560a7ffc0\", \"openrouter_claude_3_5_haiku_analysis.md\", 3038)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“„ Saved: openrouter_llama_3_1_70b_analysis.md\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_b6e78ec4-f920-40cb-8554-d5b981c26991\", \"openrouter_llama_3_1_70b_analysis.md\", 8334)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“„ Saved: openrouter_llama_3_3_70b_analysis.md\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_0a71a5f2-8261-4b04-8fd3-ba14a6a17a0b\", \"openrouter_llama_3_3_70b_analysis.md\", 4207)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“„ Saved: openrouter_gpt_4o_mini_analysis.md\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_500f406b-570a-4286-8a3e-4d6c4de12eaa\", \"openrouter_gpt_4o_mini_analysis.md\", 7012)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“„ Saved: openrouter_qwen_2_5_72b_analysis.md\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_c757b49e-de5d-4b4a-a053-88fc0cedd93c\", \"openrouter_qwen_2_5_72b_analysis.md\", 16008)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“„ Saved: openrouter_deepseek_chat_analysis.md\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_01d0c7c8-66ed-40a3-b563-3a9d637aa4a9\", \"openrouter_deepseek_chat_analysis.md\", 6476)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "ğŸ“„ Saved: hf_llama_analysis.md\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_f5246547-e734-461b-83c5-8cd2ed1f0626\", \"hf_llama_analysis.md\", 5488)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "============================================================\n",
            "ğŸ“ˆ MODEL PERFORMANCE SUMMARY\n",
            "============================================================\n",
            "\n",
            "âš¡ Fastest to Slowest:\n",
            "1. Llama 3.1 70B (HuggingFace): 11.54s\n",
            "2. Llama 3.1 70B (OpenRouter): 16.97s\n",
            "3. Claude 3.5 Haiku (OpenRouter): 18.18s\n",
            "4. GPT-4o Mini (OpenRouter): 39.19s\n",
            "5. Llama 3.3 70B (OpenRouter): 46.88s\n",
            "6. DeepSeek Chat (OpenRouter): 52.02s\n",
            "7. Qwen 2.5 72B (OpenRouter): 91.20s\n",
            "\n",
            "============================================================\n",
            "RESULTS PREVIEW\n",
            "============================================================\n",
            "\n",
            "ğŸ¤– Claude 3.5 Haiku (OpenRouter)\n",
            "â±ï¸  18.18s\n",
            "------------------------------------------------------------\n",
            "A. OVERALL RANKING (Out of 10):\n",
            "\n",
            "1. Apollo (7.5/10)\n",
            "- Mostly positive reviews about patient care\n",
            "- [Good for international patients with skilled doctors]\n",
            "- [Excellent treatment and compassionate staff]\n",
            "\n",
            "2. Max (6/10)\n",
            "- Mixed reviews with more negative experiences\n",
            "- [Some positive staff interactions]\n",
            "- [Significant complaints about billing and negligence]\n",
            "\n",
            "3. AIIMS (5.5/10)\n",
            "- Generally positive ins...\n",
            "\n",
            "\n",
            "ğŸ¤– Llama 3.1 70B (OpenRouter)\n",
            "â±ï¸  16.97s\n",
            "------------------------------------------------------------\n",
            "A. OVERALL RANKING (1-7):\n",
            "\n",
            "1. Max Hospital (Score: 6.5/10) - \"The staff here is extremely professional, caring, and polite. From the reception to the doctors, everyone ensures that patients feel comfortable and well looked after.\" [Max Hospital, MouthShut]\n",
            "2. AIIMS (Score: 6.2/10) - \"Aiims Hospital, Madurai has cutting edge technology and the skills of the doctors are also too good here.\" [AIIMS, ...\n",
            "\n",
            "\n",
            "ğŸ¤– Llama 3.3 70B (OpenRouter)\n",
            "â±ï¸  46.88s\n",
            "------------------------------------------------------------\n",
            "### A. OVERALL RANKING (1-7)\n",
            "\n",
            ". **AIIMS** - Score: 8/10 [The staff at Aiims Hospital, Madurai really helped me quite a lot. They helped me get to the toilet and do my exercises that I was recommended and they were invested in me getting alright like I was a part of their family.]\n",
            "2. **Max** - Score: 7.5/10 [I had a really positive experience at Max Hospital. The staff here is extremely professiona...\n",
            "\n",
            "\n",
            "ğŸ¤– GPT-4o Mini (OpenRouter)\n",
            "â±ï¸  39.19s\n",
            "------------------------------------------------------------\n",
            "Based on the collected data of reviews and news for the hospitals in Delhi, hereâ€™s a comprehensive analysis.\n",
            "\n",
            "### A. Overall Ranking (1-7)\n",
            "\n",
            "1. **Max Hospital** - **Score: 8.5/10**\n",
            "   * **Justification:** Despite some negative experiences, many reviews highlight positive aspects such as professionalism and attentive staff. \"The staff here is extremely professional, caring, and polite... Highly reco...\n",
            "\n",
            "\n",
            "ğŸ¤– Qwen 2.5 72B (OpenRouter)\n",
            "â±ï¸  91.20s\n",
            "------------------------------------------------------------\n",
            "### A. OVERALL RANKING (1-7):\n",
            "\n",
            "1. **AIIMS** - **Score: 9/10**\n",
            "   - **Justification:** \"This hospital has doctors who are committed to taking care of the health-related problems of the patients and they go way beyond their means to give you good services.\" [MouthShut, AIIMS]\n",
            "   - \"The staff at Aiims Hospital, Madurai really helped me quite a lot. They helped me get to the toilet and do my exercises...\n",
            "\n",
            "\n",
            "ğŸ¤– DeepSeek Chat (OpenRouter)\n",
            "â±ï¸  52.02s\n",
            "------------------------------------------------------------\n",
            "### **A. OVERALL RANKING (1-7)**  \n",
            "\n",
            "**1. AIIMS**  \n",
            "- **Score: 9/10**  \n",
            "- **Justification**: Consistently praised for expertise, affordability, and patient care. [\"All the doctors at this hospital have good years of experience so you can definitely trust them without fear.\"] [\"This hospital has doctors that are internationally renowned, yet they do not charge a bomb for the rooms and the service.\"]...\n",
            "\n",
            "\n",
            "ğŸ¤– Llama 3.1 70B (HuggingFace)\n",
            "â±ï¸  11.54s\n",
            "------------------------------------------------------------\n",
            "**A. OVERALL RANKING (1-7)**\n",
            "\n",
            "1. AIIMS (8/10) - [The staff at Aiims Hospital, Madurai really helped me quite a lot. They helped me get to the toilet and do my exercises that I was recommended and they were invested in me getting alright like I was a part of their family.]\n",
            "2. Max (7.5/10) - [The staff here is extremely professional, caring, and polite. From the reception to the doctors, everyone en...\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "from groq import Groq\n",
        "from huggingface_hub import InferenceClient\n",
        "import requests\n",
        "import json\n",
        "from datetime import datetime\n",
        "import time\n",
        "from google.colab import userdata, files\n",
        "\n",
        "# Configure APIs\n",
        "GOOGLE_API_KEY = userdata.get('GOOGLE_API_KEY')\n",
        "GROQ_API_KEY = userdata.get('GROQ_API_KEY')\n",
        "OPENROUTER_API_KEY = userdata.get('OPENROUTER_API_KEY')\n",
        "HUGGINGFACE_API_KEY = userdata.get('HUGGINGFACE_API_KEY')\n",
        "\n",
        "# Initialize clients\n",
        "genai.configure(api_key=GOOGLE_API_KEY)\n",
        "groq_client = Groq(api_key=GROQ_API_KEY)\n",
        "hf_client = InferenceClient(token=HUGGINGFACE_API_KEY)\n",
        "\n",
        "results = {}\n",
        "errors = {}\n",
        "\n",
        "print(\"ğŸ¥ MULTI-LLM HOSPITAL ANALYSIS STARTING...\\n\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# Function to create a smaller, focused prompt\n",
        "def create_focused_prompt(full_data):\n",
        "    \"\"\"Create a smaller prompt by summarizing data\"\"\"\n",
        "\n",
        "    # Extract just the key information\n",
        "    hospitals = list(full_data['data'].keys())\n",
        "\n",
        "    # Build a condensed version with sample reviews only\n",
        "    condensed_data = {}\n",
        "    for hospital, data in full_data['data'].items():\n",
        "        condensed_data[hospital] = {\n",
        "            'review_count': data['review_count'],\n",
        "            'news_count': data['news_count'],\n",
        "            'sample_reviews': data['reviews'],\n",
        "            'sample_news': data['news']\n",
        "        }\n",
        "\n",
        "    prompt = f\"\"\"I have collected review and news data for {len(hospitals)} hospitals in Delhi:\n",
        "{', '.join(hospitals)}\n",
        "\n",
        "DATA SUMMARY:\n",
        "{chr(10).join([f\"{h}: {condensed_data[h]['review_count']} reviews, {condensed_data[h]['news_count']} news\" for h in hospitals])}\n",
        "\n",
        "SAMPLE DATA:\n",
        "{json.dumps(condensed_data, indent=2)}\n",
        "\n",
        "Based on this sample data, provide:\n",
        "\n",
        "A. OVERALL RANKING (1-{len(hospitals)}):\n",
        "- Rank hospitals best to worst\n",
        "- Give each a score out of 10\n",
        "- Justify with specific review quotes in [brackets]\n",
        "\n",
        "B. TOP 3 STRENGTHS & WEAKNESSES for each hospital:\n",
        "- Use actual review quotes as evidence\n",
        "\n",
        "C. BEST HOSPITAL FOR:\n",
        "- Emergency care\n",
        "- Planned surgery\n",
        "- Budget patients\n",
        "- Cancer treatment\n",
        "- Heart disease\n",
        "- Orthopedics\n",
        "\n",
        "D. KEY RECOMMENDATIONS:\n",
        "- Which hospitals to prefer for specific needs\n",
        "- Which to avoid and why\n",
        "\n",
        "Be specific and quote actual reviews in [brackets].\"\"\"\n",
        "\n",
        "    return prompt\n",
        "\n",
        "# Helper function for OpenRouter API calls\n",
        "def call_openrouter(model_id, model_name, prompt, max_tokens=4000, timeout=120):\n",
        "    \"\"\"Generic function to call OpenRouter API\"\"\"\n",
        "    try:\n",
        "        print(f\"\\nğŸ¤– Running: {model_name}...\")\n",
        "        start = time.time()\n",
        "\n",
        "        response = requests.post(\n",
        "            url=\"https://openrouter.ai/api/v1/chat/completions\",\n",
        "            headers={\n",
        "                \"Authorization\": f\"Bearer {OPENROUTER_API_KEY}\",\n",
        "                \"Content-Type\": \"application/json\",\n",
        "                \"HTTP-Referer\": \"https://github.com/yourusername\",\n",
        "                \"X-Title\": \"Hospital Analysis\"\n",
        "            },\n",
        "            json={\n",
        "                \"model\": model_id,\n",
        "                \"messages\": [{\"role\": \"user\", \"content\": prompt}],\n",
        "                \"max_tokens\": max_tokens\n",
        "            },\n",
        "            timeout=timeout\n",
        "        )\n",
        "\n",
        "        response.raise_for_status()\n",
        "        response_data = response.json()\n",
        "\n",
        "        elapsed_time = time.time() - start\n",
        "        result = {\n",
        "            'response': response_data['choices'][0]['message']['content'],\n",
        "            'time': elapsed_time,\n",
        "            'model': model_name\n",
        "        }\n",
        "        print(f\"âœ… {model_name} completed in {elapsed_time:.2f}s\")\n",
        "        return result\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"âŒ {model_name} failed: {e}\")\n",
        "        raise e\n",
        "\n",
        "# Load your full data\n",
        "print(\"ğŸ“ Upload your combined hospital data JSON file:\")\n",
        "uploaded = files.upload()\n",
        "data_file = list(uploaded.keys())[0]\n",
        "\n",
        "with open(data_file, 'r', encoding='utf-8') as f:\n",
        "    full_hospital_data = json.load(f)\n",
        "\n",
        "# Create focused prompt\n",
        "analysis_prompt = create_focused_prompt(full_hospital_data)\n",
        "print(f\"\\nâœ… Created focused prompt: ~{len(analysis_prompt)} characters\")\n",
        "print(f\"Estimated tokens: ~{len(analysis_prompt.split())}\")\n",
        "\n",
        "# 1. GEMINI 1.5 FLASH\n",
        "try:\n",
        "    print(\"\\nğŸ¤– Running: Gemini 1.5 Flash...\")\n",
        "    start = time.time()\n",
        "\n",
        "    model = genai.GenerativeModel('gemini-1.5-flash')\n",
        "    response = model.generate_content(analysis_prompt)\n",
        "\n",
        "    results['gemini_flash'] = {\n",
        "        'response': response.text,\n",
        "        'time': time.time() - start,\n",
        "        'model': 'Gemini 1.5 Flash (Google)'\n",
        "    }\n",
        "    print(f\"âœ… Gemini Flash completed in {results['gemini_flash']['time']:.2f}s\")\n",
        "\n",
        "except Exception as e:\n",
        "    errors['gemini_flash'] = str(e)\n",
        "    print(f\"âŒ Gemini Flash failed: {e}\")\n",
        "\n",
        "# 2. GROQ - Llama 3.1 70B Versatile\n",
        "try:\n",
        "    print(\"\\nğŸ¤– Running: Groq Llama 3.1 70B...\")\n",
        "    start = time.time()\n",
        "\n",
        "    response = groq_client.chat.completions.create(\n",
        "        model=\"llama-3.1-70b-versatile\",\n",
        "        messages=[{\"role\": \"user\", \"content\": analysis_prompt}],\n",
        "        temperature=0.7,\n",
        "        max_tokens=4000\n",
        "    )\n",
        "\n",
        "    results['groq_llama_70b'] = {\n",
        "        'response': response.choices[0].message.content,\n",
        "        'time': time.time() - start,\n",
        "        'model': 'Llama 3.1 70B (Groq)'\n",
        "    }\n",
        "    print(f\"âœ… Groq Llama 70B completed in {results['groq_llama_70b']['time']:.2f}s\")\n",
        "\n",
        "except Exception as e:\n",
        "    errors['groq_llama_70b'] = str(e)\n",
        "    print(f\"âŒ Groq Llama 70B failed: {e}\")\n",
        "\n",
        "# 3. GROQ - Llama 3.2 90B\n",
        "try:\n",
        "    print(\"\\nğŸ¤– Running: Groq Llama 3.2 90B...\")\n",
        "    start = time.time()\n",
        "\n",
        "    response = groq_client.chat.completions.create(\n",
        "        model=\"llama-3.2-90b-text-preview\",\n",
        "        messages=[{\"role\": \"user\", \"content\": analysis_prompt}],\n",
        "        temperature=0.7,\n",
        "        max_tokens=4000\n",
        "    )\n",
        "\n",
        "    results['groq_llama_90b'] = {\n",
        "        'response': response.choices[0].message.content,\n",
        "        'time': time.time() - start,\n",
        "        'model': 'Llama 3.2 90B (Groq)'\n",
        "    }\n",
        "    print(f\"âœ… Groq Llama 90B completed in {results['groq_llama_90b']['time']:.2f}s\")\n",
        "\n",
        "except Exception as e:\n",
        "    errors['groq_llama_90b'] = str(e)\n",
        "    print(f\"âŒ Groq Llama 90B failed: {e}\")\n",
        "\n",
        "# OPENROUTER MODELS (EXPANDED SECTION)\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸŒ OPENROUTER MODELS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# OpenRouter Model Definitions\n",
        "openrouter_models = [\n",
        "    # Anthropic Models\n",
        "    (\"anthropic/claude-3.5-sonnet\", \"Claude 3.5 Sonnet\"),\n",
        "    (\"anthropic/claude-3.5-haiku\", \"Claude 3.5 Haiku\"),\n",
        "    (\"anthropic/claude-3-opus\", \"Claude 3 Opus\"),\n",
        "    (\"anthropic/claude-3-sonnet\", \"Claude 3 Sonnet\"),\n",
        "\n",
        "    # Meta Llama Models\n",
        "    (\"meta-llama/llama-3.1-405b-instruct\", \"Llama 3.1 405B\"),\n",
        "    (\"meta-llama/llama-3.1-70b-instruct\", \"Llama 3.1 70B\"),\n",
        "    (\"meta-llama/llama-3.3-70b-instruct\", \"Llama 3.3 70B\"),\n",
        "\n",
        "    # Google Models\n",
        "    (\"google/gemini-pro-1.5\", \"Gemini Pro 1.5\"),\n",
        "    (\"google/gemini-flash-1.5\", \"Gemini Flash 1.5\"),\n",
        "\n",
        "    # OpenAI Models\n",
        "    (\"openai/gpt-4-turbo\", \"GPT-4 Turbo\"),\n",
        "    (\"openai/gpt-4o\", \"GPT-4o\"),\n",
        "    (\"openai/gpt-4o-mini\", \"GPT-4o Mini\"),\n",
        "\n",
        "    # Mistral Models\n",
        "    (\"mistralai/mistral-large\", \"Mistral Large\"),\n",
        "    (\"mistralai/mistral-medium\", \"Mistral Medium\"),\n",
        "\n",
        "    # Other Notable Models\n",
        "    (\"qwen/qwen-2.5-72b-instruct\", \"Qwen 2.5 72B\"),\n",
        "    (\"deepseek/deepseek-chat\", \"DeepSeek Chat\"),\n",
        "    (\"cohere/command-r-plus\", \"Cohere Command R+\"),\n",
        "    (\"perplexity/llama-3.1-sonar-large-128k-online\", \"Perplexity Sonar Large\"),\n",
        "]\n",
        "\n",
        "# Run each OpenRouter model\n",
        "for model_id, model_name in openrouter_models:\n",
        "    key = f\"openrouter_{model_name.lower().replace(' ', '_').replace('.', '_').replace('-', '_').replace('+', 'plus')}\"\n",
        "    try:\n",
        "        results[key] = call_openrouter(\n",
        "            model_id=model_id,\n",
        "            model_name=f\"{model_name} (OpenRouter)\",\n",
        "            prompt=analysis_prompt\n",
        "        )\n",
        "    except Exception as e:\n",
        "        errors[key] = str(e)\n",
        "\n",
        "# 4. HUGGINGFACE - Qwen 2.5 72B\n",
        "try:\n",
        "    print(\"\\nğŸ¤– Running: HuggingFace Qwen 2.5 72B...\")\n",
        "    start = time.time()\n",
        "\n",
        "    response = hf_client.chat_completion(\n",
        "        model=\"Qwen/Qwen2.5-72B-Instruct\",\n",
        "        messages=[{\"role\": \"user\", \"content\": analysis_prompt}],\n",
        "        max_tokens=4000\n",
        "    )\n",
        "\n",
        "    results['hf_qwen'] = {\n",
        "        'response': response.choices[0].message.content,\n",
        "        'time': time.time() - start,\n",
        "        'model': 'Qwen 2.5 72B (HuggingFace)'\n",
        "    }\n",
        "    print(f\"âœ… HuggingFace Qwen completed in {results['hf_qwen']['time']:.2f}s\")\n",
        "\n",
        "except Exception as e:\n",
        "    errors['hf_qwen'] = str(e)\n",
        "    print(f\"âŒ HuggingFace Qwen failed: {e}\")\n",
        "\n",
        "# 5. HUGGINGFACE - Meta Llama 3.1 70B\n",
        "try:\n",
        "    print(\"\\nğŸ¤– Running: HuggingFace Llama 3.1 70B...\")\n",
        "    start = time.time()\n",
        "\n",
        "    response = hf_client.chat_completion(\n",
        "        model=\"meta-llama/Llama-3.1-70B-Instruct\",\n",
        "        messages=[{\"role\": \"user\", \"content\": analysis_prompt}],\n",
        "        max_tokens=4000\n",
        "    )\n",
        "\n",
        "    results['hf_llama'] = {\n",
        "        'response': response.choices[0].message.content,\n",
        "        'time': time.time() - start,\n",
        "        'model': 'Llama 3.1 70B (HuggingFace)'\n",
        "    }\n",
        "    print(f\"âœ… HuggingFace Llama completed in {results['hf_llama']['time']:.2f}s\")\n",
        "\n",
        "except Exception as e:\n",
        "    errors['hf_llama'] = str(e)\n",
        "    print(f\"âŒ HuggingFace Llama failed: {e}\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“Š ANALYSIS COMPLETE!\")\n",
        "print(f\"âœ… Successful: {len(results)} models\")\n",
        "print(f\"âŒ Failed: {len(errors)} models\")\n",
        "\n",
        "if errors:\n",
        "    print(\"\\nâš ï¸ Failed models:\")\n",
        "    for model, error in errors.items():\n",
        "        print(f\"  - {model}: {error[:100]}...\")\n",
        "\n",
        "# Save results\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "output_data = {\n",
        "    'timestamp': timestamp,\n",
        "    'successful_models': len(results),\n",
        "    'failed_models': len(errors),\n",
        "    'results': results,\n",
        "    'errors': errors,\n",
        "    'prompt_used': analysis_prompt\n",
        "}\n",
        "\n",
        "output_file = f'hospital_analysis_{timestamp}.json'\n",
        "with open(output_file, 'w', encoding='utf-8') as f:\n",
        "    json.dump(output_data, f, indent=2, ensure_ascii=False)\n",
        "\n",
        "print(f\"\\nğŸ’¾ Results saved to: {output_file}\")\n",
        "files.download(output_file)\n",
        "\n",
        "# Save individual markdown reports\n",
        "for model_name, data in results.items():\n",
        "    filename = f\"{model_name}_analysis.md\"\n",
        "    with open(filename, 'w', encoding='utf-8') as f:\n",
        "        f.write(f\"# Hospital Analysis - {data['model']}\\n\\n\")\n",
        "        f.write(f\"**Generated:** {timestamp}\\n\")\n",
        "        f.write(f\"**Processing Time:** {data['time']:.2f}s\\n\\n\")\n",
        "        f.write(\"---\\n\\n\")\n",
        "        f.write(data['response'])\n",
        "    print(f\"ğŸ“„ Saved: {filename}\")\n",
        "    files.download(filename)\n",
        "\n",
        "# Print summary statistics\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"ğŸ“ˆ MODEL PERFORMANCE SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "if results:\n",
        "    sorted_results = sorted(results.items(), key=lambda x: x[1]['time'])\n",
        "    print(\"\\nâš¡ Fastest to Slowest:\")\n",
        "    for i, (model_name, data) in enumerate(sorted_results, 1):\n",
        "        print(f\"{i}. {data['model']}: {data['time']:.2f}s\")\n",
        "\n",
        "# Print preview\n",
        "print(\"\\n\" + \"=\"*60)\n",
        "print(\"RESULTS PREVIEW\")\n",
        "print(\"=\"*60)\n",
        "for model_name, data in results.items():\n",
        "    print(f\"\\nğŸ¤– {data['model']}\")\n",
        "    print(f\"â±ï¸  {data['time']:.2f}s\")\n",
        "    print(\"-\"*60)\n",
        "    print(data['response'][:400] + \"...\\n\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
